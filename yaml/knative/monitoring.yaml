apiVersion: v1
kind: Namespace
metadata:
  labels:
    serving.knative.dev/release: "v0.10.0"
  name: knative-monitoring

---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: elasticsearch-logging
    kubernetes.io/cluster-service: "true"
    kubernetes.io/name: Elasticsearch
  name: elasticsearch-logging
  namespace: knative-monitoring
spec:
  ports:
  - port: 9200
    protocol: TCP
    targetPort: db
  selector:
    app: elasticsearch-logging
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app: elasticsearch-logging
    kubernetes.io/cluster-service: "true"
  name: elasticsearch-logging
  namespace: knative-monitoring
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: elasticsearch-logging
    kubernetes.io/cluster-service: "true"
  name: elasticsearch-logging
rules:
- apiGroups:
  - ""
  resources:
  - services
  - namespaces
  - endpoints
  verbs:
  - get
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: elasticsearch-logging
    kubernetes.io/cluster-service: "true"
  name: elasticsearch-logging
  namespace: knative-monitoring
roleRef:
  apiGroup: ""
  kind: ClusterRole
  name: elasticsearch-logging
subjects:
- apiGroup: ""
  kind: ServiceAccount
  name: elasticsearch-logging
  namespace: knative-monitoring
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app: elasticsearch-logging
    kubernetes.io/cluster-service: "true"
    version: v5.6.4
  name: elasticsearch-logging
  namespace: knative-monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: elasticsearch-logging
      version: v5.6.4
  serviceName: elasticsearch-logging
  template:
    metadata:
      labels:
        app: elasticsearch-logging
        kubernetes.io/cluster-service: "true"
        version: v5.6.4
    spec:
      containers:
      - env:
        - name: ES_JAVA_OPTS
          value: "-Xms512m -Xmx512m"
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        image: k8s.gcr.io/elasticsearch:v5.6.4
        name: elasticsearch-logging
        ports:
        - containerPort: 9200
          name: db
          protocol: TCP
        - containerPort: 9300
          name: transport
          protocol: TCP
        resources:
          limits:
            cpu: 200m
          requests:
            cpu: 100m
        volumeMounts:
        - mountPath: /data
          name: elasticsearch-logging
      initContainers:
      - command:
        - /sbin/sysctl
        - -w
        - vm.max_map_count=262144
        image: alpine:3.6
        name: elasticsearch-logging-init
        securityContext:
          privileged: true
      serviceAccountName: elasticsearch-logging
      volumes:
      - emptyDir: {}
        name: elasticsearch-logging

---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: kibana-logging
    kubernetes.io/cluster-service: "true"
    kubernetes.io/name: Kibana
  name: kibana-logging
  namespace: knative-monitoring
spec:
  ports:
  - port: 5601
    protocol: TCP
    targetPort: ui
  selector:
    app: kibana-logging
  type: NodePort
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: kibana-logging
    kubernetes.io/cluster-service: "true"
  name: kibana-logging
  namespace: knative-monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kibana-logging
  template:
    metadata:
      labels:
        app: kibana-logging
    spec:
      containers:
      - env:
        - name: ELASTICSEARCH_URL
          value: http://elasticsearch-logging:9200
        - name: SERVER_BASEPATH
          value: /api/v1/namespaces/knative-monitoring/services/kibana-logging/proxy
        - name: XPACK_MONITORING_ENABLED
          value: "false"
        - name: XPACK_SECURITY_ENABLED
          value: "false"
        image: docker.elastic.co/kibana/kibana:5.6.4
        name: kibana-logging
        ports:
        - containerPort: 5601
          name: ui
          protocol: TCP
        resources:
          limits:
            cpu: 200m
          requests:
            cpu: 100m

---
apiVersion: v1
data:
  100.system.conf: |-
    <system>
      root_dir /tmp/fluentd-buffers/
    </system>
  200.containers.input.conf: |-
    # Capture logs from container's stdout/stderr -> Docker -> .log in JSON format
    <source>
      @id containers-stdout-stderr
      @type tail
      path /var/log/containers/*user-container-*.log,/var/log/containers/*build-step-*.log,/var/log/containers/controller-*controller-*.log,/var/log/containers/webhook-*webhook-*.log,/var/log/containers/*autoscaler-*autoscaler-*.log,/var/log/containers/*queue-proxy-*.log,/var/log/containers/activator-*activator-*.log
      pos_file /var/log/containers-stdout-stderr.pos
      time_format %Y-%m-%dT%H:%M:%S.%NZ
      tag raw.kubernetes.*
      format json
      read_from_head true
    </source>
    # Capture logs from Knative containers' /var/log
    <source>
      @id containers-var-log
      @type tail
      # **/*/**/* allows path expansion to go through one symlink (the one created by the init container)
      path /var/lib/kubelet/pods/*/volumes/kubernetes.io~empty-dir/knative-internal/**/*/**/*
      path_key stream
      pos_file /var/log/containers-var-log.pos
      tag raw.kubernetes.*
      message_key log
      read_from_head true
      <parse>
        @type multi_format
        <pattern>
          format json
          time_key fluentd-time # fluentd-time is reserved for structured logs
          time_format %Y-%m-%dT%H:%M:%S.%NZ
        </pattern>
        <pattern>
          format none
          message_key log
        </pattern>
      </parse>
    </source>
    # Combine multi line logs which form an exception stack trace into a single log entry
    <match raw.kubernetes.**>
      @id raw.kubernetes
      @type detect_exceptions
      remove_tag_prefix raw
      message log
      stream stream
      multiline_flush_interval 5
      max_bytes 500000
      max_lines 1000
    </match>
    # Make stream path correct from the container's point of view
    <filter kubernetes.var.lib.kubelet.pods.*.volumes.kubernetes.io~empty-dir.knative-internal.*.**>
      @type record_transformer
      enable_ruby true
      <record>
        stream /var/log/${record["stream"].scan(/\/knative-internal\/[^\/]+\/(.*)/).last.last}
      </record>
    </filter>
    # Add Kubernetes metadata to logs from /var/log/containers
    <filter kubernetes.var.log.containers.**>
      @type kubernetes_metadata
    </filter>
    # Add Kubernetes metadata to logs from /var/lib/kubelet/pods/*/volumes/kubernetes.io~empty-dir/knative-internal/**/*/**/*
    <filter kubernetes.var.lib.kubelet.pods.**>
      @type kubernetes_metadata
      tag_to_kubernetes_name_regexp (?<docker_id>[a-z0-9]{8}-[a-z0-9]{4}-[a-z0-9]{4}-[a-z0-9]{4}-[a-z0-9]{12})\.volumes.kubernetes\.io~empty-dir\.knative-internal\.(?<namespace>[^_]+)_(?<pod_name>[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*)_(?<container_name>user-container)\..*?$
    </filter>
  300.forward.input.conf: |-
    # Takes the messages sent over TCP, e.g. request logs from Istio
    <source>
      @type forward
      port 24224
    </source>
  900.output.conf: |-
    # Send to Elastic Search
    <match **>
      @id elasticsearch
      @type elasticsearch
      @log_level info
      host elasticsearch-logging
      port 9200
      logstash_format true
      <buffer>
        @type file
        path /var/log/fluentd-buffers/kubernetes.system.buffer
        flush_mode interval
        retry_type exponential_backoff
        flush_thread_count 2
        flush_interval 5s
        retry_forever
        retry_max_interval 30
        chunk_limit_size 2M
        queue_limit_length 8
        overflow_action block
      </buffer>
    </match>
kind: ConfigMap
metadata:
  labels:
    serving.knative.dev/release: "v0.10.0"
  name: fluentd-ds-config
  namespace: knative-monitoring

---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app: fluentd-ds
    serving.knative.dev/release: "v0.10.0"
  name: fluentd-ds
  namespace: knative-monitoring
---
